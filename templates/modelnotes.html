<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>Model Notes</title>
        <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
        <script src="{{ url_for('static', filename='script.js') }}" defer></script>
    </head>
    <body>
        <div class="topnav">
            <div class="nav-container">
                <a href="/">About</a>
                <a href="/modelnotes" class="active">Model Notes</a>
                <a href="/politics">r/politics</a>
                <a href="/AmITheAsshole">r/AITA</a>
                <div class="search-container">
                    <input type="text" id="subredditSearch" class="search-input" placeholder="Enter subreddit name...">
                    <button onclick="searchSubreddit()" class="search-btn">Go</button>
                </div>
            </div>
        </div>

        <div class="container">
            <h1>Model Notes</h1>

            <h2>Model 4: DistilBERT (Hugging Face Endpoint + Redis)</h2>
            <p>This is the production model. DistilBERT is fine-tuned for sentiment classification and deployed on Hugging Face Inference Endpoints. 
               Redis is layered on top as a caching layer: users instantly see cached results (stale up to 1h old) while new predictions are fetched in the background. 
               There&apos;s also a <code>/warm</code> route to pre-heat the endpoint to reduce cold-start latency.</p>
            <p>Scores are normalized to the same [-1, 1] range as earlier models, so the front end doesn&apos;t need changes. Accuracy and general performance are way stronger than earlier setups, and it&apos;s scalable enough for a web demo.</p>
            <p>To do: experiment with larger Reddit-specific fine-tuning datasets, optimize Hugging Face instance costs, maybe add smarter eviction rules for Redis cache.</p>

            <h2>Model 3: PyTorch LSTM</h2>
            <p>Instead of pooling, this model used an LSTM, a type of RNN designed to capture sequential context across word sequences. 
               This was an improvement because word order matters a lot in sentiment (e.g., "not good" vs "good"). 
               More computationally intensive but captured more complex dependencies.</p>
            <h3>Optimization</h3>
            <p>At first, training accuracy increased and validation loss decreased until ~epoch 3. After that, validation loss went up even as training accuracy climbed, which is textbook overfitting. 
               Increasing vocabulary size (from 4k → 30k-50k) massively helped, pushing validation accuracy to ~89% at epoch 6 before overfitting. 
               Still, inference latency made it hard to serve in real-time.</p>
            <img src="{{ url_for('static', filename='epochtest1.png') }}" alt="Validation Loss vs Epoch" width="500" height="400">
            <p>Sample run:</p>
            <p>
                Epoch 1/10, Train Acc: 0.6205, Val Loss: 0.5736 <br>
                Epoch 2/10, Train Acc: 0.7090, Val Loss: 0.5674 <br>
                Epoch 3/10, Train Acc: 0.7794, Val Loss: 0.4797 <br>
                Epoch 4/10, Train Acc: 0.8460, Val Loss: 0.4006 <br>
                Epoch 5/10, Train Acc: 0.8739, Val Loss: 0.3753 <br>
                Epoch 6/10, Train Acc: 0.8944, Val Loss: 0.3733 <br>
                Epoch 7/10, Train Acc: 0.9095, Val Loss: 0.3747 <br>
                Epoch 8/10, Train Acc: 0.9193, Val Loss: 0.3974 <br>
                Epoch 9/10, Train Acc: 0.9336, Val Loss: 0.3659 <br>
                Epoch 10/10, Train Acc: 0.9483, Val Loss: 0.3954
            </p>
            <p>I planned more optimizer tuning and cross-validation runs, but honestly: LSTMs are slow, so I shelved it for production.</p>

            <h2>Model 2: Keras Neural Network</h2>
            <p>This model used an Embedding layer and a shallow Dense neural net. It was a step up from Naive Bayes since embeddings encode semantic meaning, but it ignored sequential context. 
               For example, “not good” and “good not” collapsed into the same embedding vector, which is obviously wrong. 
               Dropout helped with overfitting but overall performance plateaued quickly.</p>

            <h2>Model 1: Naive Bayes (TextBlob)</h2>
            <p>The very first prototype. TextBlob’s Naive Bayes sentiment classifier was used because it was light and easy to deploy. 
               It just counted words (“stupid” = negative, “great” = positive), but assumed independence. So “pretty stupid” = pretty + stupid instead of context. 
               Worked for a quick demo, but way too naive to be accurate.</p>

            <h2>Creator(s)</h2>
            <ul>
                <li><a href="https://github.com/alissawu">Alissa Wu</a>, New York University Computer Science and Mathematics</li>
                <li><a href="https://github.com/EvanPFWang">Evan Wang</a>, New York University Mathematics and Computer Science</li>
            </ul>
        </div>
    </body>
</html>
