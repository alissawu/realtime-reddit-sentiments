<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>Model Notes</title>
        <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
        <script src="{{ url_for('static', filename='script.js') }}" defer></script>
    </head>
    <body>
        <div class="topnav">
            <a href="/" class="{{ 'active' if active_link == 'index' else '' }}">About</a>
            <a href="/modelnotes" class="{{ 'active' if active_link == 'modelnotes' else '' }}">Model Notes</a>
            <a href="/politics" class="{{ 'active' if active_link == 'politics' else '' }}">r/politics</a>
            <a href="/AmITheAsshole" class="{{ 'active' if active_link == 'AmITheAsshole' else '' }}">r/AITA</a>
        </div>

        <div class="container">
            <h1>Model Notes</h1>
            <h2>Model 1: Naive Bayes</h2>
            <p>For fast deployment, we initially used TextBlob, which is a simple Naive Bayes sentiment model. Naive Bayes is a simple probablistic algorithm. It basically calculates the probability of a sentence being negative based on whether the words in that sentence typically appear in negative or positive sentences.</p>
            <p>For example, if 80% of training data with the word "stupid" end up being negative, "stupid" will be classified as negative, and any test sentence with the word "stupid" will have that negative weight</p>
            <p>The test sentence sentiment is based on which sentiment the words most lean toward</p>
            <p> It's a naive algorithm because it assumes word meanings are independent, which isn't that true. For example, "pretty" is a posoitive word, "stupid" is a negative word, but "pretty stupid" is especially negative.</p>
            <p> It's light and easy to deploy'</p>
            
            <h2>Model 2: Keras Neural Network Model</h2>
            <p>This model used an Embedding layer and a simple Dense neural network to classify sentiment. It captures some word relationships via embeddings but doesn't model temporal dependencies</p>
            <h3>Architecture</h3>
                <p>Embedding layer: turns word indicies into dense word vectors, which captures semantic meanings, and doesn't treat words as independent</p>
                <p>GlobalAveragePooling1D: Summarizes the word embeddings of a sentence into a single vector.</p>
                <p>Dense layers: Fully connected layers that learn to map the summarized vector to a sentiment prediction.</p>
                <p>Dropout layer: Helps prevent overfitting by randomly deactivating some neurons during training.</p>
            <p>Word embedddings allows the model to have a strong understanding of words compared to Naive Bayes</p>
            <p>However, it lacks a sequential context (word order), so like "not good" and "good not" would mean the same thing</p>
            <p>Also doesn't model how a word can change on its surrounding context"</p>
            
            <h2>Model 3: PyTorch</h2>
            
            
            <h2>Model 3 Optimization</h2>
            <img src="{{ url_for('static', filename='0accuracyvsepoch.png') }}" alt="Accuracy vs Epoch" width="500" height="400">

            <p>Up to E3, train and validation accuracy increases</p>
            <p>After that, train accuracy improves, but validation decreases, which indicates overfitting. So 3 epochs is the best</p>
            <p>The confusion matrix (haven't done this yet, more analysis and improvement later)</p>
            
            <ul>
                <li>Displays mean and median sentiment statistics and formatted data of the 50 posts.</li>
            </ul>

            <h2>Creator(s)</h2>
            <ul>
                <li><a href="https://github.com/alissawu">Alissa Wu</a>, Duke University Computer Science and Mathematics</li>
                <li><a href="https://github.com/EvanPFWang">Evan Wang</a>, New York University Mathematics and Computer Science</li>
            </ul>
            <p>(Evan pls add to this when you're free)</p>
        </div>
    </body>
</html>
